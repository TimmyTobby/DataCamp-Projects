{"cells":[{"source":"![Facial Recognition](facialrecognition.jpg)","metadata":{},"id":"88c639dc-bfcd-449e-bddd-55f8bd551dd5","cell_type":"markdown"},{"source":"You are a member of an elite group of data scientists, specialising in advanced facial recognition technology, this firm is dedicated to identifying and safeguarding prominent individuals from various spheresâ€”ranging from entertainment and sports to politics and philanthropy. The team's mission is to deploy AI-driven solutions that can accurately distinguish between images of notable personalities and the general populace, enhancing the personal security of such high-profile individuals. You're to focus on Arnold Schwarzenegger, a figure whose accomplishments span from bodybuilding champion to Hollywood icon, and from philanthropist to the Governor of California. ","metadata":{},"id":"0fd04b96-d360-411c-8b17-a10382c97d29","cell_type":"markdown"},{"source":"### **The Data**\nThe `data/lfw_arnie_nonarnie.csv` dataset contains processed facial image data derived from the \"Labeled Faces in the Wild\" (LFW) dataset, focusing specifically on images of Arnold Schwarzenegger and other individuals not identified as him. This dataset has been prepared to aid in the development and evaluation of facial recognition models. There are 40 images of Arnold Schwarzenegger and 150 of other people.\n\n| Column Name | Description |\n|-------------|-------------|\n| PC1, PC2, ... PCN | Principal components from PCA, capturing key image features. |\n| Label | Binary indicator: `1` for Arnold Schwarzenegger, `0` for others. |","metadata":{},"id":"be124832-8192-4f93-b487-247c3d03d23b","cell_type":"markdown"},{"source":"## Data Preparation\n- Dataset: lfw_arnie_nonarnie.csv\n- Predictors: All columns except Label\n- Class Label: Label\n\n## Data Splitting\n- Method: Train-test split\n- Test Size: 20%\n- Random State: 21\n- Stratify: By Label to ensure balanced classes","metadata":{},"cell_type":"markdown","id":"881d4f4a-8482-487c-b946-b1dcde014c77"},{"source":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read the CSV file \ndf = pd.read_csv(\"data/lfw_arnie_nonarnie.csv\")\n\n# Seperate the predictor and class label\nX = df.drop('Label', axis=1)\ny = df['Label'] \n\n# Split the data into training and testing sets using stratify to balance the class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1717891661257,"lastExecutedByKernel":"d330418a-209a-48b0-ae20-536c0b58a856","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read the CSV file \ndf = pd.read_csv(\"data/lfw_arnie_nonarnie.csv\")\n\n# Seperate the predictor and class label\nX = df.drop('Label', axis=1)\ny = df['Label'] \n\n# Split the data into training and testing sets using stratify to balance the class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"},"id":"2e0be28b-29dd-49bd-8e8b-f3cb828a5065","cell_type":"code","execution_count":84,"outputs":[]},{"source":"## Model Selection and Training\n### Models Considered\n- Random Forest Classifier\n- K-Nearest Neighbors\n- Logistic Regression\n\n### Preprocessing\n- StandardScaler: Used to standardize features\n\n### Cross-Validation\n- Method: K-Fold Cross-Validation\n- Number of Splits: 15\n- Shuffle: True\n- Random State: 21\n\n### Pipeline and hyperparameter tuning\n- Steps: Scaling and Classification\n- Grid Search\n     -- Estimator: Pipeline\n     -- Scoring: Accuracy\n\n","metadata":{},"cell_type":"markdown","id":"a317b10a-b294-4718-9772-7535e46af04f"},{"source":"# Choosing models\nrf = RandomForestClassifier()\nlogreg = LogisticRegression()\nknn = KNeighborsClassifier()\nscaler = StandardScaler()\n# Model list\nmodels = {'RandomForestClassifier' : rf, \n          'KNeighborsClassifier' : knn,\n          'Logistic Regression' : logreg \n         }\n\nkf = KFold(n_splits=15, shuffle=True, random_state=21)\nsteps = [('scaler', scaler), ('classifier', rf)] # Default classifier, can be changed during GridSearchCV\npipeline = Pipeline(steps)\nparams = [\n    {\n        'classifier' : [rf],\n        'classifier__criterion' : ['gini', 'entropy'], # HyperparameterS for rf\n        'classifier__min_samples_leaf' : [1, 5, 10],\n        'classifier__n_estimators' : [50, 100, 150],\n        'classifier__max_depth': [None, 10, 20] ,\n        'classifier__random_state' : [21],\n        'classifier__n_jobs' : [-1]\n    },\n    {\n        'classifier' : [knn],                       # Hyperparameters for KNN\n        'classifier__leaf_size' : [10, 30, 50],\n        'classifier__n_neighbors' : [1, 5, 10],\n        'classifier__n_jobs' : [-1]\n    },\n    {\n        'classifier' : [logreg],                   # Hyperparameters for Logistic Regression\n        'classifier__C' : [0.1, 0.5, 1.0],\n        'classifier__solver' : ['lbfgs', 'liblinear', 'saga'],\n        'classifier__max_iter' : [50, 100, 150],\n        'classifier__random_state' : [21],\n        'classifier__n_jobs' : [-1]\n    }\n] \n# Performing GridSearch Cross Validation\ngrid_cv = GridSearchCV(estimator= pipeline, param_grid= params, cv=kf, n_jobs=-1, scoring='accuracy')\ngrid_cv.fit(X_train, y_train)\n# Selecting the best model\nbest_model = grid_cv.best_estimator_\nprint(best_model)","metadata":{"executionCancelledAt":null,"executionTime":110664,"lastExecutedAt":1717891771921,"lastExecutedByKernel":"d330418a-209a-48b0-ae20-536c0b58a856","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\nrf = RandomForestClassifier()\nlogreg = LogisticRegression()\nknn = KNeighborsClassifier()\nscaler = StandardScaler()\n# Model list\nmodels = {'RandomForestClassifier' : rf, \n          'KNeighborsClassifier' : knn,\n          'Logistic Regression' : logreg \n         }\n\nkf = KFold(n_splits=15, shuffle=True, random_state=21)\nsteps = [('scaler', scaler), ('classifier', rf)] # Default classifier, can be changed during GridSearchCV\npipeline = Pipeline(steps)\nparams = [\n    {\n        'classifier' : [rf],\n        'classifier__criterion' : ['gini', 'entropy'], # Hyperparameter for rf\n        'classifier__min_samples_leaf' : [1, 5, 10],\n        'classifier__n_estimators' : [50, 100, 150],\n        'classifier__max_depth': [None, 10, 20] ,\n        'classifier__random_state' : [21],\n        'classifier__n_jobs' : [-1]\n    },\n    {\n        'classifier' : [knn],\n        'classifier__leaf_size' : [10, 30, 50],\n        'classifier__n_neighbors' : [1, 5, 10],\n        'classifier__n_jobs' : [-1]\n    },\n    {\n        'classifier' : [logreg],\n        'classifier__C' : [0.1, 0.5, 1.0],\n        'classifier__solver' : ['lbfgs', 'liblinear', 'saga'],\n        'classifier__max_iter' : [50, 100, 150],\n        'classifier__random_state' : [21],\n        'classifier__n_jobs' : [-1]\n    }\n] \n\ngrid_cv = GridSearchCV(estimator= pipeline, param_grid= params, cv=kf, n_jobs=-1, scoring='accuracy')\ngrid_cv.fit(X_train, y_train)\nbest_model = grid_cv.best_estimator_","outputsMetadata":{"0":{"height":249,"type":"stream"}}},"id":"f09a4e74-4a69-4dfc-8a54-f285653796d9","cell_type":"code","execution_count":85,"outputs":[]},{"source":"## Model Evaluation\n### Best Model\n- Model: Logistic Regression\n- Best Hyperparameters:\n- Test Set Evaluation, F1 Score, Recall, Precision and Accuracy.\n\nPredicted Labels: Generated using the best model","metadata":{},"cell_type":"markdown","id":"f31d9697-09a8-4e1b-a74e-d68dcc929a5d"},{"source":"# Calculating accuracy and Making predictions \nbest_model_info = grid_cv.best_params_\nbest_model_cv_score = grid_cv.best_score_\ny_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\nbest_model_name = \"Logistic Regression\"","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1717891772069,"lastExecutedByKernel":"d330418a-209a-48b0-ae20-536c0b58a856","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(best_model)\nbest_model_info = grid_cv.best_params_\nbest_model_cv_score = grid_cv.best_score_\ny_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\naccuracy\nbest_model_name = \"Logistic Regression\"","outputsMetadata":{"0":{"height":102,"type":"stream"}}},"cell_type":"code","id":"9746a9ed-be96-4226-9f29-447b303890b6","outputs":[{"output_type":"stream","name":"stdout","text":"Pipeline(steps=[('scaler', StandardScaler()),\n                ('classifier',\n                 LogisticRegression(C=0.5, max_iter=50, n_jobs=-1,\n                                    random_state=21, solver='saga'))])\n"}],"execution_count":87},{"source":"# Claculating Precision, Recall, F1 score\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nscore = accuracy\nprint(f\"The best model is {best_model_name}\", '\\n',\n     f\"The Precision score is: {precision}\", '\\n', \n     f\"The Accuracy score is: {accuracy}\", '\\n',\n     f\"The recall score is: {recall}\", '\\n',\n     f\"The F1 score is: {f1}\", '\\n',\n     f\"The best model parameter is:\\n {best_model_info}\", '\\n', \n     f\"The best cross validation score is: {best_model_cv_score}\") ","metadata":{"executionCancelledAt":null,"executionTime":58,"lastExecutedAt":1717891772127,"lastExecutedByKernel":"d330418a-209a-48b0-ae20-536c0b58a856","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"precision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nscore = accuracy\nprint(f\"The best model is {best_model_name}\", '\\n',\n     f\"The Precision score is: {precision}\", '\\n', \n     f\"The Accuracy score is: {accuracy}\", '\\n',\n     f\"The recall score is: {recall}\", '\\n',\n     f\"The F1 score is: {f1}\", '\\n',\n     f\"The best model parameter is:\\n {best_model_info}\", '\\n', \n     f\"The best cross validation score is: {best_model_cv_score}\") ","outputsMetadata":{"0":{"height":228,"type":"stream"}}},"cell_type":"code","id":"35670994-b08f-428d-8dfb-69f5b65e59a0","outputs":[{"output_type":"stream","name":"stdout","text":"The best model is Logistic Regression \n The Precision score is: 1.0 \n The Accuracy score is: 0.8157894736842105 \n The recall score is: 0.125 \n The F1 score is: 0.2222222222222222 \n The best model parameter is:\n {'classifier': LogisticRegression(C=0.5, max_iter=50, n_jobs=-1, random_state=21,\n                   solver='saga'), 'classifier__C': 0.5, 'classifier__max_iter': 50, 'classifier__n_jobs': -1, 'classifier__random_state': 21, 'classifier__solver': 'saga'} \n The best cross validation score is: 0.8157575757575759\n"}],"execution_count":88}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}